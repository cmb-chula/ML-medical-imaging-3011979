{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fallen-spider",
   "metadata": {
    "id": "fallen-spider"
   },
   "source": [
    "# Continuing from last time, we will try to improve performance on our CXR dataset using a DenseNet model that have been pretrained on CXR\n",
    "## Preparation instructions\n",
    "1. Make sure you have a Google account\n",
    "2. Copy **this notebook** and **L12_pretrained_densenet121_NIH.h5** files to the root directory of your Google Drive\n",
    "\n",
    "## Remember to change RUNTIME TYPE to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-detector",
   "metadata": {
    "id": "metallic-detector"
   },
   "source": [
    "## To mount your Google Drive to Google Colab, run the command below and follow the instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-punch",
   "metadata": {
    "id": "responsible-punch"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urX2W7Ulq_ng",
   "metadata": {
    "id": "urX2W7Ulq_ng"
   },
   "source": [
    "## Check GPU status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r0lmg3WQh3ip",
   "metadata": {
    "id": "r0lmg3WQh3ip"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RR13kqBG1At6",
   "metadata": {
    "id": "RR13kqBG1At6"
   },
   "source": [
    "## Install UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O6_Uwg171Ev0",
   "metadata": {
    "id": "O6_Uwg171Ev0"
   },
   "outputs": [],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-container",
   "metadata": {
    "id": "surrounded-container"
   },
   "source": [
    "## Import libraries\n",
    "Google Colab should already have these basic libraries installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-explosion",
   "metadata": {
    "id": "employed-explosion"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "import umap\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qWy1Sx1Bu5bI",
   "metadata": {
    "id": "qWy1Sx1Bu5bI"
   },
   "source": [
    "## First, let's recap on the two ways to create model in TensorFlow/Keras\n",
    "**tensorflow.keras.models.Model** and **tensorflow.keras.models.Sequential**\n",
    "\n",
    "### With Sequential interface, we define a model and then add layers to it one a time in sequential order\n",
    "Note that the input is **layers.InputLayer(input_shape = (128, 128, 1))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84I4ljd3u5vC",
   "metadata": {
    "id": "84I4ljd3u5vC"
   },
   "outputs": [],
   "source": [
    "def generate_sequential_model():\n",
    "    model = Sequential()\n",
    "    model.add(layers.InputLayer(input_shape = (128, 128, 1)))\n",
    "\n",
    "    model.add(layers.Conv2D(filters = 64, kernel_size = [5, 5], padding = 'valid', strides = [2, 2]))\n",
    "    model.add(layers.Activation(activation = 'relu'))\n",
    "\n",
    "    model.add(layers.Conv2D(filters = 128, kernel_size = [3, 3], padding = 'valid', strides = [1, 1]))\n",
    "    model.add(layers.Activation(activation = 'relu'))\n",
    "    model.add(layers.MaxPool2D(pool_size = [3, 3], strides = [2, 2]))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(units = 256, activation = 'relu'))\n",
    "    model.add(layers.Dense(units = 256, activation = 'relu'))\n",
    "    model.add(layers.Dense(units = 7, activation = 'softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eczMoAYqvXtJ",
   "metadata": {
    "id": "eczMoAYqvXtJ"
   },
   "outputs": [],
   "source": [
    "ex_sequential_model = generate_sequential_model()\n",
    "ex_sequential_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AEpLVQLVvfZg",
   "metadata": {
    "id": "AEpLVQLVvfZg"
   },
   "source": [
    "### With Model interface, we connect layers manually and then call Model() to designate the input and output layers\n",
    "Note that the input is **layers.Input(shape = (128, 128, 1))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A9RwJGECvfmg",
   "metadata": {
    "id": "A9RwJGECvfmg"
   },
   "outputs": [],
   "source": [
    "def generate_model():\n",
    "    inputs = layers.Input(shape = (128, 128, 1))\n",
    "    x = layers.Conv2D(filters = 64, kernel_size = [5, 5], padding = 'valid', strides = [2, 2])(inputs)\n",
    "    x = layers.Activation(activation = 'relu')(x)\n",
    "    \n",
    "    x = layers.Conv2D(filters = 128, kernel_size = [3, 3], padding = 'valid', strides = [1, 1])(x)\n",
    "    x = layers.Activation(activation = 'relu')(x)\n",
    "    x = layers.MaxPool2D(pool_size = [3, 3], strides = [2, 2])(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    x = layers.Dense(units = 256, activation = 'relu')(x)\n",
    "    x = layers.Dense(units = 256, activation = 'relu')(x)\n",
    "    outputs = layers.Dense(units = 7, activation = 'softmax')(x)\n",
    "\n",
    "    return Model(inputs = inputs, outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4VYB4eL7wp77",
   "metadata": {
    "id": "4VYB4eL7wp77"
   },
   "outputs": [],
   "source": [
    "ex_model = generate_model()\n",
    "ex_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X9Fcz76NxcED",
   "metadata": {
    "id": "X9Fcz76NxcED"
   },
   "source": [
    "### The power of Model over Sequential is that we can define model with multiple branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Cc1FDt4AxckQ",
   "metadata": {
    "id": "Cc1FDt4AxckQ"
   },
   "outputs": [],
   "source": [
    "def generate_multi_branch_model():\n",
    "    inputs1 = layers.Input(shape = (128, 128, 1), name = 'image_data_input')\n",
    "    x1 = layers.Conv2D(filters = 64, kernel_size = [5, 5], padding = 'valid', strides = [2, 2])(inputs1)\n",
    "    x1 = layers.Activation(activation = 'relu')(x1) \n",
    "    x1 = layers.Conv2D(filters = 128, kernel_size = [3, 3], padding = 'valid', strides = [1, 1])(x1)\n",
    "    x1 = layers.Activation(activation = 'relu')(x1)\n",
    "    x1 = layers.MaxPool2D(pool_size = [3, 3], strides = [2, 2])(x1)\n",
    "    x1 = layers.Flatten()(x1)\n",
    "\n",
    "    inputs2 = layers.Input(shape = (100), name = 'clinical_data_input')\n",
    "    x2 = layers.Dense(units = 128, activation = 'relu')(inputs2)\n",
    "\n",
    "    merged = layers.Concatenate()([x1, x2])\n",
    "\n",
    "    merged1 = layers.Dense(units = 256, activation = 'relu')(merged)\n",
    "    merged1 = layers.Dense(units = 256, activation = 'relu')(merged1)\n",
    "    outputs1 = layers.Dense(units = 7, activation = 'softmax', name = 'classification_output')(merged1)\n",
    "\n",
    "    merged2 = layers.Dense(units = 128, activation = 'relu')(merged)\n",
    "    outputs2 = layers.Dense(units = 1, activation = 'linear', name = 'regression_output')(merged2)\n",
    "\n",
    "    return Model(inputs = [inputs1, inputs2], outputs = [outputs1, outputs2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EKfWEVJAywbk",
   "metadata": {
    "id": "EKfWEVJAywbk"
   },
   "outputs": [],
   "source": [
    "ex_multi_branch_model = generate_multi_branch_model()\n",
    "ex_multi_branch_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mhSZr6O90NiQ",
   "metadata": {
    "id": "mhSZr6O90NiQ"
   },
   "source": [
    "## We can use plot_model function from tensorflow.keras.utils to get a clearer picture of this architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OszLjLBQ0N0w",
   "metadata": {
    "id": "OszLjLBQ0N0w"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(ex_multi_branch_model, to_file = 'model.png', show_shapes = True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-thanksgiving",
   "metadata": {
    "id": "interracial-thanksgiving"
   },
   "source": [
    "## Now, let's get back to our chest x-ray data\n",
    "This file is in a compressed **h5** format. We can read it with library **h5py** which is already available on Google Colab server\n",
    "\n",
    "### We will extract only images with single diagnosis label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-atmosphere",
   "metadata": {
    "id": "eligible-atmosphere"
   },
   "outputs": [],
   "source": [
    "cxr_data = []\n",
    "cxr_label = []\n",
    "\n",
    "with h5py.File('/content/drive/MyDrive/L11.2_CXR_data.h5', 'r') as data:\n",
    "    findings = [x.decode('UTF-8') for x in data['Finding Labels']]\n",
    "    images = data['images']\n",
    "    \n",
    "    for i in range(len(findings)):\n",
    "        if len(findings[i].split('|')) == 1: ## single diagnosis\n",
    "            if findings[i] == '':\n",
    "                cxr_label.append('Normal')\n",
    "            else:\n",
    "                cxr_label.append(findings[i])\n",
    "            \n",
    "            cxr_data.append(images[i] / 255.0) ## divide by 255.0\n",
    "            \n",
    "cxr_data = np.array(cxr_data)\n",
    "cxr_label = np.array(cxr_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-eating",
   "metadata": {
    "id": "entire-eating"
   },
   "source": [
    "### The image data has already been formatted for CNN input (notice the last channel dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-composite",
   "metadata": {
    "id": "collaborative-composite"
   },
   "outputs": [],
   "source": [
    "print('data dimension:', cxr_data.shape)\n",
    "print('label dimension:', cxr_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-undergraduate",
   "metadata": {
    "id": "dried-undergraduate"
   },
   "source": [
    "### We just need to format the label into one-hot code\n",
    "Use **sklearn.preprocessing** module's **OneHotEncoder()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-investing",
   "metadata": {
    "id": "first-investing"
   },
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse = False).fit(cxr_label.reshape(-1, 1)) ## reshape(-1, 1) to convert 1D vector into 2D\n",
    "cxr_label_onehot = encoder.transform(cxr_label.reshape(-1, 1))\n",
    "\n",
    "print('onehot label dimension:', cxr_label_onehot.shape)\n",
    "print('categories:', encoder.categories_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-exhibition",
   "metadata": {
    "id": "final-exhibition"
   },
   "source": [
    "### There are 15 classes: 14 diagnosis + normal\n",
    "Let's look at the class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-asbestos",
   "metadata": {
    "id": "hungarian-asbestos"
   },
   "outputs": [],
   "source": [
    "h, bins = np.histogram(np.argmax(cxr_label_onehot, axis = 1), bins = cxr_label_onehot.shape[1])\n",
    "plt.bar(range(cxr_label_onehot.shape[1]), h)\n",
    "plt.xticks(range(cxr_label_onehot.shape[1]), labels = encoder.categories_[0], rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-leisure",
   "metadata": {
    "id": "descending-leisure"
   },
   "source": [
    "### Some classes, like Hernia and Pneumonia, have very few images\n",
    "## Let's select classes with plenty of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EzIGFavH1BTz",
   "metadata": {
    "id": "EzIGFavH1BTz"
   },
   "outputs": [],
   "source": [
    "selected_classes = ['Atelectasis', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Normal', 'Pneumothorax']\n",
    "selected_classes_cols = [i for i in range(len(encoder.categories_[0])) if encoder.categories_[0][i] in selected_classes]\n",
    "\n",
    "selected_label_onehot = cxr_label_onehot[:, selected_classes_cols]\n",
    "selected_row = selected_label_onehot.sum(axis = 1) == 1\n",
    "\n",
    "selected_data = cxr_data[selected_row, :]\n",
    "selected_label_onehot = selected_label_onehot[selected_row, :]\n",
    "\n",
    "print('selected data dimension:', selected_data.shape)\n",
    "print('selected label dimension:', selected_label_onehot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z944tZgT2-HZ",
   "metadata": {
    "id": "z944tZgT2-HZ"
   },
   "source": [
    "## Duplicate the data to create 3-channel inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rqYCRdCV2-YA",
   "metadata": {
    "id": "rqYCRdCV2-YA"
   },
   "outputs": [],
   "source": [
    "selected_data_3ch = np.concatenate([selected_data, selected_data, selected_data], axis = 3)\n",
    "print(selected_data_3ch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-multimedia",
   "metadata": {
    "id": "intimate-multimedia"
   },
   "source": [
    "## Split selected data into 60-20-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-glucose",
   "metadata": {
    "id": "provincial-glucose"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(selected_data_3ch, selected_label_onehot, test_size = 0.2, stratify = selected_label_onehot, \\\n",
    "                                                    shuffle = True, random_state = 3011979)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, stratify = y_train, \\\n",
    "                                                  shuffle = True, random_state = 3011979)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KRMuRiWo3QP4",
   "metadata": {
    "id": "KRMuRiWo3QP4"
   },
   "source": [
    "## Delete unused variables to save RAM space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "He8fPIBW3Qzw",
   "metadata": {
    "id": "He8fPIBW3Qzw"
   },
   "outputs": [],
   "source": [
    "del selected_data\n",
    "del selected_data_3ch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-egN3xyLrbSX",
   "metadata": {
    "id": "-egN3xyLrbSX"
   },
   "source": [
    "## Create a DenseNet121 model that fits the pretrained model's description\n",
    "The model was trained on 14-class CXR dataset using the following architecture\n",
    "1. DenseNet121\n",
    "2. GlobalAveragePooling2D\n",
    "3. Dense with 1024 units and ReLU activation\n",
    "4. BatchNormalization\n",
    "5. Dropout with rate 0.2\n",
    "6. Dense with 512 units and ReLU activation\n",
    "7. BatchNormalization\n",
    "8. Dropout with rate 0.2\n",
    "9. Dense with 14 units and sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Qr3f-xHTrbk4",
   "metadata": {
    "id": "Qr3f-xHTrbk4"
   },
   "outputs": [],
   "source": [
    "def generate_cxr_pretrained_densenet():\n",
    "    base_model = DenseNet121(input_shape = (128, 128, 3), weights = None, classes = 14, include_top = False)\n",
    "    base_model.trainable = False ## freeze convolutional part\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = layers.Dense(1024, activation = 'relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(512, activation = 'relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(14, activation = 'sigmoid')(x)\n",
    "    model = Model(inputs = base_model.input, outputs = x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w1Q3NixQs8io",
   "metadata": {
    "id": "w1Q3NixQs8io"
   },
   "outputs": [],
   "source": [
    "cxr_pretrained_densenet = generate_cxr_pretrained_densenet()\n",
    "cxr_pretrained_densenet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fUHz8nNWtkKS",
   "metadata": {
    "id": "fUHz8nNWtkKS"
   },
   "source": [
    "## After confirming the architecture, let's load the pretrained weights from **densenet_121_ADAM_14_class_lr=0.01_avgpool_224_NIH.h5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KfR5_ahftkZg",
   "metadata": {
    "id": "KfR5_ahftkZg"
   },
   "outputs": [],
   "source": [
    "cxr_pretrained_densenet.load_weights('/content/drive/MyDrive/L12_pretrained_densenet121_NIH.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5IB0ozDtRa5",
   "metadata": {
    "id": "f5IB0ozDtRa5"
   },
   "source": [
    "## However, this model architecture doesn't exactly match what we want\n",
    "Our task only contain 7 outputs, not 14\n",
    "\n",
    "## We can access each **layer** in the model through **model.layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ql4Q3akftRy4",
   "metadata": {
    "id": "Ql4Q3akftRy4"
   },
   "outputs": [],
   "source": [
    "print('Layer at position 0:', cxr_pretrained_densenet.layers[0].name, cxr_pretrained_densenet.layers[0])\n",
    "print('Layer at position -2:', cxr_pretrained_densenet.layers[-2].name, cxr_pretrained_densenet.layers[-2])\n",
    "print('Layer at position -1:', cxr_pretrained_densenet.layers[-1].name, cxr_pretrained_densenet.layers[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2kt9tw8kuSYo",
   "metadata": {
    "id": "2kt9tw8kuSYo"
   },
   "source": [
    "## Let's create a new model that is identical to the pretrained one, except that the output layer changes from Dense(14) to Dense(7)\n",
    "The trick is to connect the last Dropout layer (at position -2) to a new Dense(7) layer\n",
    "\n",
    "Note that we are also changing the formulation of CXR classification from **multi-class classification** to **multi-label classification**. This changes the final layer's activation from **softmax** to **sigmoid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mmb75qjcuSvJ",
   "metadata": {
    "id": "mmb75qjcuSvJ"
   },
   "outputs": [],
   "source": [
    "new_output = layers.Dense(7, activation = 'sigmoid')(cxr_pretrained_densenet.layers[-2].output)\n",
    "new_cxr_pretrained_densenet = Model(inputs = cxr_pretrained_densenet.input, outputs = new_output)\n",
    "new_cxr_pretrained_densenet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Tw_IrMxR1-4A",
   "metadata": {
    "id": "Tw_IrMxR1-4A"
   },
   "source": [
    "### Note that the parent model's weights and settings are inherited\n",
    "\n",
    "## Let's do some inference using the loaded models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gKTt3Jew188B",
   "metadata": {
    "id": "gKTt3Jew188B"
   },
   "outputs": [],
   "source": [
    "y_val_pred = new_cxr_pretrained_densenet.predict(X_val)\n",
    "\n",
    "for i in range(len(selected_classes)):\n",
    "    print('class:', selected_classes[i], 'accuracy:', accuracy_score(y_val[:, i], y_val_pred[:, i] > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FbcvV-1U4aOZ",
   "metadata": {
    "id": "FbcvV-1U4aOZ"
   },
   "source": [
    "## Define callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7WR_zE-64EZo",
   "metadata": {
    "id": "7WR_zE-64EZo"
   },
   "outputs": [],
   "source": [
    "modelckpt_callback = ModelCheckpoint('/content/drive/MyDrive/densenet_cxr_L12_checkpoint/', \\\n",
    "                                     verbose = 1, save_best_only = True, save_weights_only = True)\n",
    "tensorboard_callback = TensorBoard('/content/drive/MyDrive/tb_log_L12', histogram_freq = 1)\n",
    "reduced_lr_callback = ReduceLROnPlateau(patience = 3, factor = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s-QyzGwl4kLX",
   "metadata": {
    "id": "s-QyzGwl4kLX"
   },
   "source": [
    "## Start TensorBoard iinstance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hc40bfnL4jPB",
   "metadata": {
    "id": "Hc40bfnL4jPB"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gKtz9jVQm9wt",
   "metadata": {
    "id": "gKtz9jVQm9wt"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir /content/drive/MyDrive/tb_log_L12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h-ZwjLkq-G-6",
   "metadata": {
    "id": "h-ZwjLkq-G-6"
   },
   "source": [
    "## Define ImageDataGenerator to apply some augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZhqcxVj4-HTJ",
   "metadata": {
    "id": "ZhqcxVj4-HTJ"
   },
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range = 10,\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    )\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wDCZ_2tX5d8w",
   "metadata": {
    "id": "wDCZ_2tX5d8w"
   },
   "source": [
    "## Compile and train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WbE-7Ae65eZg",
   "metadata": {
    "id": "WbE-7Ae65eZg"
   },
   "outputs": [],
   "source": [
    "new_cxr_pretrained_densenet.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tNXk4nJL5pNQ",
   "metadata": {
    "id": "tNXk4nJL5pNQ"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(3011979)\n",
    "history = new_cxr_pretrained_densenet.fit(datagen.flow(X_train, y_train, batch_size = 128), steps_per_epoch = X_train.shape[0] / 128,\n",
    "                                          validation_data = (X_val, y_val), epochs = 20,\n",
    "                                          callbacks = [modelckpt_callback, tensorboard_callback, reduced_lr_callback], verbose = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PDIJWM2L5pCf",
   "metadata": {
    "id": "PDIJWM2L5pCf"
   },
   "source": [
    "## There is no good built-in metric for multi-label classification, so we can only monitor the loss values\n",
    "\n",
    "## Evaluate model performance on the validation set again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sVLDd3vS6H3o",
   "metadata": {
    "id": "sVLDd3vS6H3o"
   },
   "outputs": [],
   "source": [
    "y_val_pred = new_cxr_pretrained_densenet.predict(X_val)\n",
    "\n",
    "for i in range(len(selected_classes)):\n",
    "    print('class:', selected_classes[i], 'accuracy:', accuracy_score(y_val[:, i], y_val_pred[:, i] > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y-N2CJ5r91ox",
   "metadata": {
    "id": "y-N2CJ5r91ox"
   },
   "source": [
    "## And on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oSksdLwiF-gS",
   "metadata": {
    "id": "oSksdLwiF-gS"
   },
   "outputs": [],
   "source": [
    "y_test_pred = new_cxr_pretrained_densenet.predict(X_test)\n",
    "\n",
    "for i in range(len(selected_classes)):\n",
    "    print('class:', selected_classes[i], 'accuracy:', accuracy_score(y_test[:, i], y_test_pred[:, i] > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k9_hBRB7yg_R",
   "metadata": {
    "id": "k9_hBRB7yg_R"
   },
   "source": [
    "## Next, let's explore the embedding of CXR images learned by our model\n",
    "This is the output of the **global_average_pooling2d** layer in our model\n",
    "\n",
    "### To obtain the output of this specific layer, we use the same trick as when we replace Dense(14) with dense(7)\n",
    "Create a new model with the same input but with output being the **global_average_pooling2d**'s output.\n",
    "\n",
    "We can directly call a specific layer using its name via **model.get_layer(name = '')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S9RtGbWDzqtH",
   "metadata": {
    "id": "S9RtGbWDzqtH"
   },
   "outputs": [],
   "source": [
    "gap_output = new_cxr_pretrained_densenet.get_layer(name = 'global_average_pooling2d').output\n",
    "embedding_model = Model(inputs = new_cxr_pretrained_densenet.input, outputs = gap_output)\n",
    "\n",
    "X_test_embedding = embedding_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "djevQ4dW0rY9",
   "metadata": {
    "id": "djevQ4dW0rY9"
   },
   "source": [
    "### The embedding should have dimension = (number of test samples, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZvaZftUN0mVd",
   "metadata": {
    "id": "ZvaZftUN0mVd"
   },
   "outputs": [],
   "source": [
    "X_test_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o78_tbDX0z-1",
   "metadata": {
    "id": "o78_tbDX0z-1"
   },
   "source": [
    "## Use UMAP to visualize the embedding distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67OHmeSZ0lvt",
   "metadata": {
    "id": "67OHmeSZ0lvt"
   },
   "outputs": [],
   "source": [
    "X_test_embedding_umap = umap.UMAP(n_neighbors = 50, min_dist = 0.5, n_components = 2, metric = 'euclidean', random_state = 3011979).fit_transform(X_test_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rak17AB63ONo",
   "metadata": {
    "id": "rak17AB63ONo"
   },
   "source": [
    "### Colored by predicted probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w6SPF16y0qKm",
   "metadata": {
    "id": "w6SPF16y0qKm"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (17, 8))\n",
    "\n",
    "for i in range(len(selected_classes)):\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    plt.scatter(X_test_embedding_umap[:, 0], X_test_embedding_umap[:, 1], c = y_test_pred[:, i], cmap = 'Reds', alpha = 0.8)\n",
    "    plt.title(selected_classes[i]); plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kN07shuh3IF5",
   "metadata": {
    "id": "kN07shuh3IF5"
   },
   "source": [
    "### Colored by ground truth label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KMBO41jt3Il5",
   "metadata": {
    "id": "KMBO41jt3Il5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 8))\n",
    "\n",
    "for i in range(len(selected_classes)):\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    sns.kdeplot(x = X_test_embedding_umap[y_test[:, i] == 1, 0], y = X_test_embedding_umap[y_test[:, i] == 1, 1], fill = True)\n",
    "    plt.title(selected_classes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3011979_L13_043021_workshop_more_deep_learning_on_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

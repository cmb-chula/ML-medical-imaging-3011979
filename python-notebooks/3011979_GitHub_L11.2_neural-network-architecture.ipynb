{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "several-analyst",
   "metadata": {},
   "source": [
    "# Today, we will explore three new tools\n",
    "1. Multi-class classification\n",
    "2. Basic convolutional neural network\n",
    "3. Google Colaboratory\n",
    "\n",
    "## Fashion MNIST data will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "african-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-province",
   "metadata": {},
   "source": [
    "## Load data and extract input and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "humanitarian-holmes",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>203</td>\n",
       "      <td>214</td>\n",
       "      <td>166</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      2       0       0       0       0       0       0       0       0   \n",
       "1      9       0       0       0       0       0       0       0       0   \n",
       "2      6       0       0       0       0       0       0       0       5   \n",
       "3      0       0       0       0       1       2       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "5      4       0       0       0       5       4       5       5       3   \n",
       "6      4       0       0       0       0       0       0       0       0   \n",
       "7      5       0       0       0       0       0       0       0       0   \n",
       "8      4       0       0       0       0       0       0       3       2   \n",
       "9      8       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0        30        43         0   \n",
       "3       0  ...         3         0         0         0         0         1   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "5       5  ...         7         8         7         4         3         7   \n",
       "6       0  ...        14         0         0         0         0         0   \n",
       "7       0  ...         0         0         0         0         0         0   \n",
       "8       0  ...         1         0         0         0         0         0   \n",
       "9       0  ...       203       214       166         0         0         0   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "5         5         0         0         0  \n",
       "6         0         0         0         0  \n",
       "7         0         0         0         0  \n",
       "8         0         0         0         0  \n",
       "9         0         0         0         0  \n",
       "\n",
       "[10 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw = pd.read_csv('L11.2_mnist_fashion.csv', header = 0, index_col = False)\n",
    "data_raw.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-summer",
   "metadata": {},
   "source": [
    "### Keep only the first 1,000 data points to speed things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sustainable-designation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimension: (1000, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    9\n",
       "2    6\n",
       "3    0\n",
       "4    3\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_raw.iloc[:1000, 1:] ## input = 2nd column onward\n",
    "data = data / 255.0 ## convert [0, 255] to [0, 1]\n",
    "label = data_raw['label'].iloc[:1000] ## label = 1st column\n",
    "\n",
    "print('data dimension:', data.shape)\n",
    "\n",
    "label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-hospital",
   "metadata": {},
   "source": [
    "## First, let's look at how classical ML models handle multiple classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "juvenile-sierra",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-realtor",
   "metadata": {},
   "source": [
    "## Split data into 80-20 train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "incoming-delight",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size = 0.2, stratify = label, \\\n",
    "                                                    shuffle = True, random_state = 3011979)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-single",
   "metadata": {},
   "source": [
    "## Use default models to fit the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "stylish-newsletter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=10000, random_state=3011979)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = LogisticRegression(max_iter = 10000, random_state = 3011979)\n",
    "rf_model = RandomForestClassifier(random_state = 3011979)\n",
    "mlp_model = MLPClassifier(max_iter = 10000, random_state = 3011979)\n",
    "\n",
    "lr_model.fit(X_train, y_train)\n",
    "rf_model.fit(X_train, y_train)\n",
    "mlp_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-proof",
   "metadata": {},
   "source": [
    "## Let's look at the outputs\n",
    "### First, the class labels seen by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "demographic-barrel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression class labels: [0 1 2 3 4 5 6 7 8 9]\n",
      "Random Forest class labels: [0 1 2 3 4 5 6 7 8 9]\n",
      "MLP class labels: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print('Logistic Regression class labels:', lr_model.classes_)\n",
    "print('Random Forest class labels:', rf_model.classes_)\n",
    "print('MLP class labels:', mlp_model.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-carrier",
   "metadata": {},
   "source": [
    "### Next, the prediction outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "angry-mouth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input test data dimension: (200, 784)\n",
      "prediction output dimension: (200, 10)\n",
      "predictions:\n",
      " [[3.15901956e-02 1.19909349e-02 3.20200663e-01 ... 1.74958302e-02\n",
      "  2.05857877e-02 4.99499367e-03]\n",
      " [9.93115733e-03 1.35818114e-03 6.87123786e-01 ... 1.44330281e-03\n",
      "  4.14161185e-03 8.86683547e-04]\n",
      " [7.70752489e-05 1.77878685e-05 9.73211509e-01 ... 5.90884378e-08\n",
      "  8.80494637e-03 2.10908721e-03]\n",
      " ...\n",
      " [1.20222309e-03 1.03651906e-04 9.86887714e-01 ... 6.66103910e-09\n",
      "  4.95084733e-04 8.01956738e-05]\n",
      " [8.30846820e-05 2.21989572e-04 5.09720985e-01 ... 4.10189637e-07\n",
      "  2.63845254e-04 4.58964164e-05]\n",
      " [1.66409080e-07 2.14189289e-05 2.18711262e-05 ... 9.97829383e-01\n",
      "  4.50281438e-04 2.94399960e-05]]\n",
      "sums along class axis:\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "lr_predict = lr_model.predict_proba(X_test)\n",
    "print('input test data dimension:', X_test.shape)\n",
    "print('prediction output dimension:', lr_predict.shape)\n",
    "print('predictions:\\n', lr_predict)\n",
    "print('sums along class axis:\\n', lr_predict.sum(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "thousand-auditor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input test data dimension: (200, 784)\n",
      "prediction output dimension: (200, 10)\n",
      "predictions:\n",
      " [[0.01 0.01 0.51 ... 0.   0.04 0.  ]\n",
      " [0.11 0.04 0.22 ... 0.01 0.07 0.  ]\n",
      " [0.05 0.   0.53 ... 0.   0.05 0.01]\n",
      " ...\n",
      " [0.01 0.01 0.74 ... 0.   0.   0.  ]\n",
      " [0.01 0.   0.4  ... 0.   0.   0.  ]\n",
      " [0.   0.   0.   ... 0.91 0.   0.01]]\n",
      "sums along class axis:\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "rf_predict = rf_model.predict_proba(X_test)\n",
    "print('input test data dimension:', X_test.shape)\n",
    "print('prediction output dimension:', rf_predict.shape)\n",
    "print('predictions:\\n', rf_predict)\n",
    "print('sums along class axis:\\n', rf_predict.sum(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "chicken-detector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input test data dimension: (200, 784)\n",
      "prediction output dimension: (200, 10)\n",
      "predictions:\n",
      " [[1.92229832e-03 9.55594494e-04 2.12665513e-01 ... 5.73223661e-04\n",
      "  2.22646732e-03 1.53398724e-04]\n",
      " [5.33823698e-04 5.03099417e-05 6.34444357e-01 ... 5.63300615e-05\n",
      "  9.10632346e-04 1.68721546e-06]\n",
      " [1.53312150e-09 1.04784663e-11 9.99645095e-01 ... 2.86858903e-11\n",
      "  2.66724963e-04 9.42380045e-09]\n",
      " ...\n",
      " [6.96188433e-07 1.97544690e-12 9.99994990e-01 ... 2.01401060e-14\n",
      "  2.66637271e-08 3.56749322e-14]\n",
      " [8.83874978e-08 1.04157390e-08 8.66301511e-01 ... 3.35192282e-09\n",
      "  1.08032807e-06 4.52225424e-10]\n",
      " [8.61812708e-14 6.71531831e-10 2.11595149e-09 ... 9.99994885e-01\n",
      "  5.97613081e-08 7.73507353e-10]]\n",
      "sums along class axis:\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "mlp_predict = mlp_model.predict_proba(X_test)\n",
    "print('input test data dimension:', X_test.shape)\n",
    "print('prediction output dimension:', mlp_predict.shape)\n",
    "print('predictions:\\n', mlp_predict)\n",
    "print('sums along class axis:\\n', mlp_predict.sum(axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-honolulu",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "* Output dimension is N x L where N = number of samples and L = number of class labels\n",
    "* We use **np.argmax(, axis = 1)** to find the predicted class label as the class with highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "consolidated-organizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression confusion matrix:\n",
      " [[17  0  0  2  0  0  3  0  0  0]\n",
      " [ 0 16  0  0  1  0  1  0  0  0]\n",
      " [ 1  0 12  2  3  0  2  0  0  0]\n",
      " [ 0  0  0 17  3  0  2  0  0  0]\n",
      " [ 0  0  5  1 15  0  2  0  0  0]\n",
      " [ 1  0  0  0  0  9  0  7  0  0]\n",
      " [ 1  0  4  1  4  0 11  0  0  0]\n",
      " [ 0  0  0  0  0  1  0 17  0  2]\n",
      " [ 0  0  0  0  1  0  0  0 18  0]\n",
      " [ 0  0  1  0  0  1  0  1  0 15]]\n",
      "Random Forest confusion matrix:\n",
      " [[19  0  0  0  0  0  3  0  0  0]\n",
      " [ 0 16  1  1  0  0  0  0  0  0]\n",
      " [ 1  0 16  1  2  0  0  0  0  0]\n",
      " [ 0  0  0 20  1  0  1  0  0  0]\n",
      " [ 0  0  6  2 14  1  0  0  0  0]\n",
      " [ 0  0  0  0  0 13  0  4  0  0]\n",
      " [ 2  0  4  3  2  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  1  0 17  0  2]\n",
      " [ 0  0  1  0  0  0  0  0 18  0]\n",
      " [ 0  0  0  0  0  0  1  3  0 14]]\n",
      "MLP confusion matrix:\n",
      " [[19  0  0  0  0  0  3  0  0  0]\n",
      " [ 0 16  0  0  1  0  1  0  0  0]\n",
      " [ 0  1 13  1  3  0  2  0  0  0]\n",
      " [ 0  0  0 17  3  0  2  0  0  0]\n",
      " [ 0  0  3  1 16  0  3  0  0  0]\n",
      " [ 1  0  0  0  0  9  0  7  0  0]\n",
      " [ 1  0  3  0  6  0 11  0  0  0]\n",
      " [ 0  0  0  0  0  1  0 17  0  2]\n",
      " [ 0  0  0  0  1  0  0  0 18  0]\n",
      " [ 1  0  0  0  0  2  0  1  0 14]]\n"
     ]
    }
   ],
   "source": [
    "print('Logistic Regression confusion matrix:\\n', confusion_matrix(y_test, np.argmax(lr_predict, axis = 1)))\n",
    "print('Random Forest confusion matrix:\\n', confusion_matrix(y_test, np.argmax(rf_predict, axis = 1)))\n",
    "print('MLP confusion matrix:\\n', confusion_matrix(y_test, np.argmax(mlp_predict, axis = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-valley",
   "metadata": {},
   "source": [
    "## To monitor performance across mutliple classes, we can use a scikit-learn built-in report\n",
    "Import **classification_report()** from **sklearn.metrics**\n",
    "\n",
    "**classification_report()** takes 3 key inputs:\n",
    "1. True label\n",
    "2. Predicted label\n",
    "3. Name for each label\n",
    "\n",
    "### Define name for each class in [Fashion MNIST](https://www.bualabs.com/archives/3398/what-is-fashion-mnist-dataset/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "grand-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = {0:'T-shirt/top', 1:'Trouser', 2:'Pullover', 3:'Dress', 4:'Coat', 5:'Sandal', 6:'Shirt', \\\n",
    "              7:'Sneaker', 8:'Bag', 9:'Ankle boot'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "afraid-england",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " T-shirt/top       0.85      0.77      0.81        22\n",
      "     Trouser       1.00      0.89      0.94        18\n",
      "    Pullover       0.55      0.60      0.57        20\n",
      "       Dress       0.74      0.77      0.76        22\n",
      "        Coat       0.56      0.65      0.60        23\n",
      "      Sandal       0.82      0.53      0.64        17\n",
      "       Shirt       0.52      0.52      0.52        21\n",
      "     Sneaker       0.68      0.85      0.76        20\n",
      "         Bag       1.00      0.95      0.97        19\n",
      "  Ankle boot       0.88      0.83      0.86        18\n",
      "\n",
      "    accuracy                           0.73       200\n",
      "   macro avg       0.76      0.74      0.74       200\n",
      "weighted avg       0.75      0.73      0.74       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, np.argmax(lr_predict, axis = 1), \\\n",
    "                            target_names = [class_name[x] for x in lr_model.classes_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-heath",
   "metadata": {},
   "source": [
    "## Model coefficients (for Logistic Regression and MLP)\n",
    "* Logistic Regression generates **one coefficient set per label**\n",
    "* MLP **outputs multiple classes** within a single model\n",
    "  * Output layer size = number of class labels\n",
    "  * The connection from hidden layer (size = 100) to output layer (size = number of class labels) acts like multiple Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "informed-batman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logisit Regression coefficient counts: (10, 784)\n",
      "MLP coefficient counts:\n",
      "  Layer 0: (784, 100)\n",
      "  Layer 1: (100, 10)\n"
     ]
    }
   ],
   "source": [
    "print('Logistic Regression coefficient counts:', lr_model.coef_.shape)\n",
    "print('MLP coefficient counts:')\n",
    "\n",
    "for i in range(len(mlp_model.coefs_)):\n",
    "    print('  Layer ' + str(i) + ':', mlp_model.coefs_[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-partner",
   "metadata": {},
   "source": [
    "### To normalize output from all neuron in the output layers to sum to 1, we use [Softmax activation](https://en.wikipedia.org/wiki/Softmax_function) which converts $\\left(y_1, y_2, \\dots, y_m\\right)$ to $\\left(\\frac{e^{y_1}}{K}, \\frac{e^{y_2}}{K}, \\dots, \\frac{e^{y_m}}{K}\\right)$, where $K = \\sum_{i=1}^m e^{y_i}$\n",
    "Basically, we train the neural network model to predict values that are similar to log probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-manufacturer",
   "metadata": {},
   "source": [
    "## So far, we achieve ok performance on by considering each pixel as a separated input\n",
    "But some classes still have 50-60% precision and recall. **Can we do better with CNN on input image?**\n",
    "\n",
    "## Let's use TensorFlow/Keras to implement AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cosmetic-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Conv2D, MaxPool2D, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-civilization",
   "metadata": {},
   "source": [
    "## Original AlexNet consists of the following layers\n",
    "**First convolutional block**\n",
    "1. **Conv2D** with 96 filters of size 11x11, stride of 4x4, and valid padding\n",
    "2. **Activation** with ReLu\n",
    "\n",
    "**Second convolutional block**\n",
    "\n",
    "3. **Conv2D** with 256 filters of size 5x5, stride of 1x1, and valid padding\n",
    "4. **Activation** with ReLu\n",
    "5. **MaxPool2D** with pool size of 3x3 and stride of 2x2\n",
    "\n",
    "**Third convolutional block**\n",
    "\n",
    "6. **Conv2D** with 384 filters of size 3x3, stride of 1x1, and valid padding\n",
    "7. **Activation** with ReLu\n",
    "8. **MaxPool2D** with pool size of 3x3 and stride of 2x2\n",
    "\n",
    "**Fourth convolutional block**\n",
    "\n",
    "9. **Conv2D** with 384 filters of size 3x3, stride of 1x1, and same padding\n",
    "10. **Activation** with ReLu\n",
    "\n",
    "**Fifth convolutional block**\n",
    "\n",
    "11. **Conv2D** with 256 filters of size 3x3, stride of 1x1, and same padding\n",
    "12. **Activation** with ReLu\n",
    "13. **MaxPool2D** with pool size of 3x3 and stride of 2x2\n",
    "\n",
    "**Feed into fully connected layers**\n",
    "\n",
    "14. **Flatten**\n",
    "15. **Dense** with 4096 hidden neurons\n",
    "16. **Dense** with 4096 hidden neurons\n",
    "17. **Dense** with 1000 output neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-briefs",
   "metadata": {},
   "source": [
    "## Let's put together the model\n",
    "### Note that the original AlexNet was designed for input dimension of 224x224 while our images are 28x28\n",
    "### We will make some adjustments to reduce the model accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "authentic-taste",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gen_alexnet():\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape = [28, 28, 1]))\n",
    "\n",
    "    model.add(Conv2D(filters = 64, kernel_size = [5, 5], padding = 'valid', strides = [2, 2]))\n",
    "    model.add(Activation(activation = 'relu'))\n",
    "\n",
    "    model.add(Conv2D(filters = 128, kernel_size = [3, 3], padding = 'valid', strides = [1, 1]))\n",
    "    model.add(Activation(activation = 'relu'))\n",
    "    model.add(MaxPool2D(pool_size = [3, 3], strides = [2, 2]))\n",
    "    \n",
    "    model.add(Conv2D(filters = 256, kernel_size = [3, 3], padding = 'same', strides = [1, 1]))\n",
    "    model.add(Activation(activation = 'relu'))\n",
    "    model.add(MaxPool2D(pool_size = [3, 3], strides = [2, 2]))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(units = 1024, activation = 'relu'))\n",
    "    model.add(Dense(units = 512, activation = 'relu'))\n",
    "    model.add(Dense(units = 10, activation = 'softmax')) ## use Softmax activation for output 10 classes\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = 0.0001), \\\n",
    "                        metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "complimentary-sensitivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_32 (Conv2D)           (None, 12, 12, 64)        1664      \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 10, 10, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,163,786\n",
      "Trainable params: 1,163,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "our_alexnet = gen_alexnet()\n",
    "our_alexnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-logistics",
   "metadata": {},
   "source": [
    "## Use the full dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "brutal-authorization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimension: (60000, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    9\n",
       "2    6\n",
       "3    0\n",
       "4    3\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_raw.iloc[:, 1:] ## input = 2nd column onward\n",
    "data = data / 255.0 ## convert [0, 255] to [0, 1]\n",
    "label = data_raw['label'] ## label = 1st column\n",
    "\n",
    "print('data dimension:', data.shape)\n",
    "label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-baltimore",
   "metadata": {},
   "source": [
    "## We need to preprocess input and label data dimension for TensorFlow/Keras\n",
    "### For input, the data dimension will be converted from 60,000 x 784 to 60,000 x 28 x 28 x 1\n",
    "Convolutional layer expect 3D input for 2D image with an extra dimension for **channel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "japanese-daisy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of new input data: (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "reshaped_data = []\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    reshaped_data.append(np.reshape(data.iloc[i, :].to_numpy(), (28, 28, 1)))\n",
    "\n",
    "reshaped_data = np.array(reshaped_data)\n",
    "print('shape of new input data:', reshaped_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-aspect",
   "metadata": {},
   "source": [
    "### For label, the model expect a one-hot encoding style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "particular-arthur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of new label data: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "reshaped_label = pd.get_dummies(label).to_numpy()\n",
    "print('shape of new label data:', reshaped_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-musician",
   "metadata": {},
   "source": [
    "## Split data into 60-20-20 train-validation-test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "marine-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reshaped_data, reshaped_label, test_size = 0.2, stratify = label, \\\n",
    "                                                    shuffle = True, random_state = 3011979)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, stratify = y_train, \\\n",
    "                                                  shuffle = True, random_state = 3011979)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "breeding-timothy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9600, 28, 28, 1)\n",
      "(9600, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-bangladesh",
   "metadata": {},
   "source": [
    "## Let's do a simple training with only ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "better-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_alexnet = tf.keras.models.load_model('./fashion_mnist_l11/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "serial-finder",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "300/300 [==============================] - 18s 58ms/step - loss: 0.3339 - accuracy: 0.8782 - val_loss: 0.3514 - val_accuracy: 0.8732\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 2/100\n",
      "300/300 [==============================] - 18s 60ms/step - loss: 0.3233 - accuracy: 0.8821 - val_loss: 0.3529 - val_accuracy: 0.8698\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 3/100\n",
      "300/300 [==============================] - 18s 61ms/step - loss: 0.3135 - accuracy: 0.8856 - val_loss: 0.3349 - val_accuracy: 0.8805\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 4/100\n",
      "300/300 [==============================] - 17s 58ms/step - loss: 0.3040 - accuracy: 0.8892 - val_loss: 0.3431 - val_accuracy: 0.8724\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 5/100\n",
      "300/300 [==============================] - 18s 59ms/step - loss: 0.2945 - accuracy: 0.8915 - val_loss: 0.3256 - val_accuracy: 0.8833\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 6/100\n",
      "300/300 [==============================] - 18s 59ms/step - loss: 0.2838 - accuracy: 0.8970 - val_loss: 0.3304 - val_accuracy: 0.8835\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 7/100\n",
      "300/300 [==============================] - 18s 59ms/step - loss: 0.2757 - accuracy: 0.8984 - val_loss: 0.3160 - val_accuracy: 0.8859\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 8/100\n",
      "300/300 [==============================] - 18s 60ms/step - loss: 0.2669 - accuracy: 0.9033 - val_loss: 0.3298 - val_accuracy: 0.8814\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 9/100\n",
      "300/300 [==============================] - 18s 61ms/step - loss: 0.2612 - accuracy: 0.9046 - val_loss: 0.3264 - val_accuracy: 0.8801\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 10/100\n",
      "300/300 [==============================] - 18s 59ms/step - loss: 0.2504 - accuracy: 0.9094 - val_loss: 0.3056 - val_accuracy: 0.8904\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 11/100\n",
      "300/300 [==============================] - 17s 57ms/step - loss: 0.2428 - accuracy: 0.9124 - val_loss: 0.3120 - val_accuracy: 0.8890\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 12/100\n",
      "300/300 [==============================] - 18s 60ms/step - loss: 0.2303 - accuracy: 0.9162 - val_loss: 0.2994 - val_accuracy: 0.8935\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 13/100\n",
      "300/300 [==============================] - 17s 57ms/step - loss: 0.2221 - accuracy: 0.9182 - val_loss: 0.3006 - val_accuracy: 0.8951\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 14/100\n",
      "300/300 [==============================] - 17s 58ms/step - loss: 0.2145 - accuracy: 0.9211 - val_loss: 0.2921 - val_accuracy: 0.8990\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 15/100\n",
      "300/300 [==============================] - 18s 59ms/step - loss: 0.2034 - accuracy: 0.9263 - val_loss: 0.2893 - val_accuracy: 0.8990\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 16/100\n",
      "300/300 [==============================] - 18s 61ms/step - loss: 0.2004 - accuracy: 0.9258 - val_loss: 0.3220 - val_accuracy: 0.8863\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 17/100\n",
      "300/300 [==============================] - 18s 58ms/step - loss: 0.1914 - accuracy: 0.9304 - val_loss: 0.2931 - val_accuracy: 0.8993\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 18/100\n",
      "300/300 [==============================] - 19s 63ms/step - loss: 0.1873 - accuracy: 0.9314 - val_loss: 0.2948 - val_accuracy: 0.8981\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 19/100\n",
      "300/300 [==============================] - 18s 59ms/step - loss: 0.1780 - accuracy: 0.9353 - val_loss: 0.3016 - val_accuracy: 0.8952\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 20/100\n",
      "300/300 [==============================] - 18s 58ms/step - loss: 0.1681 - accuracy: 0.9392 - val_loss: 0.3184 - val_accuracy: 0.8942\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 21/100\n",
      "300/300 [==============================] - 18s 59ms/step - loss: 0.1654 - accuracy: 0.9396 - val_loss: 0.2964 - val_accuracy: 0.8974\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 22/100\n",
      "300/300 [==============================] - 18s 61ms/step - loss: 0.1562 - accuracy: 0.9432 - val_loss: 0.2957 - val_accuracy: 0.8995\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 23/100\n",
      "300/300 [==============================] - 18s 59ms/step - loss: 0.1490 - accuracy: 0.9464 - val_loss: 0.3255 - val_accuracy: 0.8890\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 24/100\n",
      "300/300 [==============================] - 18s 60ms/step - loss: 0.1464 - accuracy: 0.9464 - val_loss: 0.2936 - val_accuracy: 0.9011\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 25/100\n",
      "300/300 [==============================] - 17s 58ms/step - loss: 0.1372 - accuracy: 0.9501 - val_loss: 0.3443 - val_accuracy: 0.8874\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 26/100\n",
      "300/300 [==============================] - 17s 58ms/step - loss: 0.1295 - accuracy: 0.9538 - val_loss: 0.3097 - val_accuracy: 0.8990\n",
      "INFO:tensorflow:Assets written to: ./fashion_mnist_l11\\model.ckpt\\assets\n",
      "Epoch 27/100\n",
      "124/300 [===========>..................] - ETA: 9s - loss: 0.1273 - accuracy: 0.9539"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-cffddba864bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3011979\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m history = our_alexnet.fit(X_train, y_train, batch_size = 128, epochs = 100, \\\n\u001b[0m\u001b[0;32m      3\u001b[0m                           \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                           callbacks = [ModelCheckpoint('./fashion_mnist_l11/model.ckpt', verbose = 0)])\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tf.random.set_seed(3011979)\n",
    "# history = our_alexnet.fit(X_train, y_train, batch_size = 128, epochs = 100, \\\n",
    "#                           validation_data = (X_val, y_val), verbose = 1, \\\n",
    "#                           callbacks = [ModelCheckpoint('./fashion_mnist_l11/model.ckpt', verbose = 0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-europe",
   "metadata": {},
   "source": [
    "### View loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "scheduled-distributor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (6, 5))\n",
    "plt.plot(history.history['accuracy'], label = 'train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-guest",
   "metadata": {},
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "rapid-orange",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " T-shirt/top       0.81      0.85      0.83      1200\n",
      "     Trouser       0.99      0.98      0.99      1200\n",
      "    Pullover       0.81      0.86      0.84      1200\n",
      "       Dress       0.94      0.82      0.88      1200\n",
      "        Coat       0.84      0.82      0.83      1200\n",
      "      Sandal       0.97      0.98      0.97      1200\n",
      "       Shirt       0.70      0.74      0.72      1200\n",
      "     Sneaker       0.96      0.95      0.95      1200\n",
      "         Bag       0.98      0.97      0.97      1200\n",
      "  Ankle boot       0.96      0.96      0.96      1200\n",
      "\n",
      "    accuracy                           0.89     12000\n",
      "   macro avg       0.90      0.89      0.89     12000\n",
      "weighted avg       0.90      0.89      0.89     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.argmax(y_test, axis = 1), np.argmax(our_alexnet.predict(X_test), axis = 1), \\\n",
    "                            target_names = [class_name[x] for x in lr_model.classes_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-architect",
   "metadata": {},
   "source": [
    "### It is clear that our basic PC cannot handle larger neural network and dataset\n",
    "# Time to use free online resources from Google Colaboratory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
